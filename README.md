<img src="https://github.com/kunishou/Japanese-Alpaca-LoRA/blob/main/image/top.png" alt="alpaca">

# ğŸ¦™ğŸŒ²ğŸ¤ğŸŒ¸ Japanese-Alpaca-LoRA ğŸŒ¸
Alpaca-LoRA is a {model_param}-parameter LLaMA model finetuned to follow instructions. It is trained on the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) dataset

æ—¥æœ¬èªã«ç¿»è¨³ã—ãŸStanford Alpacaã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦LLaMAã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ä½œæˆã—ãŸLow-Rank Adapterã®ãƒªãƒ³ã‚¯ã¨Generateã£ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰

### Japanese-Alpaca-LoRA-7b DEMOãƒšãƒ¼ã‚¸(æœŸé–“é™å®š)  
https://huggingface.co/spaces/kunishou/Japanese-Alapaca-LoRA-7b-DEMO

Temparature : ç”Ÿæˆã™ã‚‹å›ç­”ã®å¤šæ§˜æ€§åº¦åˆã„  
Beams : ç”Ÿæˆã™ã‚‹å›ç­”ã®å€™è£œæ•°  
max_tokens : ç”Ÿæˆã™ã‚‹å›ç­”ã®é•·ã•  

å…¥åŠ›ä¾‹ï¼š  
instruct :   
input :   

### Try the pretrainde model using google colab

<a href="https://colab.research.google.com/github/kunishou/Japanese-Alpaca-LoRA/blob/main/generate_colb.ipynb" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

### LoRA on Hugging Face
Japanese-Alpaca-LoRA 7b, 13B, 30B (65B Coming Soon!)
https://huggingface.co/kunishou

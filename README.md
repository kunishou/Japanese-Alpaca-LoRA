<img src="https://github.com/kunishou/Japanese-Alpaca-LoRA/blob/main/image/top.png" alt="alpaca">
# ğŸ¦™ğŸŒ²ğŸ¤ğŸŒ¸ Japanese-Alpaca-LoRA ğŸŒ¸
Alpaca-LoRA is a {model_param}-parameter LLaMA model finetuned to follow instructions. It is trained on the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) dataset

æ—¥æœ¬èªã«ç¿»è¨³ã—ãŸStanford Alpacaã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”¨ã„ã¦LLaMAã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—å¾—ã‚‰ã‚ŒãŸLow-Rank Adapterã®ã¾ã¨ã‚ãƒªãƒã‚¸ãƒˆãƒªã¨Generateã£ã‚µãƒ³ãƒ—ãƒ«ã‚³ãƒ¼ãƒ‰

### æ¦‚è¦

### Japanese-Alpaca-LoRA-7b DEMOãƒšãƒ¼ã‚¸(æœŸé–“é™å®š)  
https://huggingface.co/spaces/kunishou/Japanese-Alapaca-LoRA-7b-DEMO

### Try the pretrainde model using google colab


### LoRA on Hugging Face
Japanese-Alpaca-LoRA 7b, 13B, 30B (65B Coming Soon!)
https://huggingface.co/kunishou

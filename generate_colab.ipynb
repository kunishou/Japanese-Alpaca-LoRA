{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunishou/Japanese-Alpaca-LoRA/blob/main/generate_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ※ 30BモデルはMax_tokensを128にしないとエラーが出ることがあります。"
      ],
      "metadata": {
        "id": "6vhnIEzPy4HU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "REC8UsP7mr0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/kunishou/Japanese-Alpaca-LoRA.git\n",
        "%cd Japanese-Alpaca-LoRA"
      ],
      "metadata": {
        "id": "oumckS9eRAUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# パッケージのインストール\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "lTSBldIT7Y96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from peft import PeftModel\n",
        "import transformers\n",
        "import gradio as gr\n",
        "\n",
        "assert (\n",
        "    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n",
        "), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n",
        "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig\n",
        "\n",
        "# colab pro以上でのプランでA100を使用しないと動かないかも\n",
        "\n",
        "BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
        "# BASE_MODEL = \"decapoda-research/llama-13b-hf\"\n",
        "# BASE_MODEL = \"decapoda-research/llama-30b-hf\"\n",
        "# BASE_MODEL = \"decapoda-research/llama-65b-hf\"\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(BASE_MODEL,device_map={'': 0})\n",
        "\n",
        "LORA_WEIGHTS = \"kunishou/Japanese-Alpaca-LoRA-7b-v0\"\n",
        "# LORA_WEIGHTS =\"kunishou/Japanese-Alpaca-LoRA-13b-v0\"\n",
        "# LORA_WEIGHTS = \"kunishou/Japanese-Alpaca-LoRA-30b-v0\"\n",
        "# LORA_WEIGHTS = \"kunishou/Japanese-Alpaca-LoRA-65b-v0\"\n",
        "\n",
        "if BASE_MODEL == \"decapoda-research/llama-7b-hf\":\n",
        "  model_param = \"7B\"\n",
        "elif BASE_MODEL == \"decapoda-research/llama-13b-hf\":\n",
        "  model_param = \"13B\"\n",
        "elif BASE_MODEL == \"decapoda-research/llama-30b-hf\":\n",
        "  model_param = \"30B\"\n",
        "else:\n",
        "  model_param = \"65B\"\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "try:\n",
        "    if torch.backends.mps.is_available():\n",
        "        device = \"mps\"\n",
        "except:\n",
        "    pass\n",
        "\n",
        "if device == \"cuda\":\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        BASE_MODEL,\n",
        "        load_in_8bit=True,\n",
        "        torch_dtype=torch.float16,\n",
        "        # device_map=\"auto\",\n",
        "        device_map={'': 0},\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(model, LORA_WEIGHTS, torch_dtype=torch.float16, device_map={'': 0},)\n",
        "elif device == \"mps\":\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        BASE_MODEL,\n",
        "        # device_map={\"\": device},\n",
        "        device_map={'': 0},\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        LORA_WEIGHTS,\n",
        "        # device_map={\"\": device},\n",
        "        device_map={'': 0},\n",
        "        torch_dtype=torch.float16,\n",
        "    )\n",
        "else:\n",
        "    model = LlamaForCausalLM.from_pretrained(\n",
        "        BASE_MODEL,\n",
        "        # device_map={\"\": device},\n",
        "        device_map={'': 0},\n",
        "        low_cpu_mem_usage=True\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(\n",
        "        model,\n",
        "        LORA_WEIGHTS,\n",
        "        # device_map={\"\": device},\n",
        "        device_map={'': 0},\n",
        "    )\n",
        "\n",
        "\n",
        "def generate_prompt(instruction, input=None):\n",
        "    if input:\n",
        "        return f\"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "{instruction}\n",
        "### Input:\n",
        "{input}\n",
        "### Response:\"\"\"\n",
        "    else:\n",
        "        return f\"\"\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
        "### Instruction:\n",
        "{instruction}\n",
        "### Response:\"\"\"\n",
        "\n",
        "\n",
        "model.eval()\n",
        "if torch.__version__ >= \"2\":\n",
        "    model = torch.compile(model)\n",
        "\n",
        "\n",
        "def evaluate(\n",
        "    instruction,\n",
        "    input=None,\n",
        "    temperature=0.1,\n",
        "    top_p=0.75,\n",
        "    top_k=40,\n",
        "    num_beams=4,\n",
        "    max_new_tokens=256,\n",
        "    **kwargs,\n",
        "):\n",
        "    prompt = generate_prompt(instruction, input)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    input_ids = inputs[\"input_ids\"].to(device)\n",
        "    generation_config = GenerationConfig(\n",
        "        temperature=temperature,\n",
        "        top_p=top_p,\n",
        "        top_k=top_k,\n",
        "        num_beams=num_beams,\n",
        "        no_repeat_ngram_size=3,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    with torch.no_grad():\n",
        "        generation_output = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            generation_config=generation_config,\n",
        "            return_dict_in_generate=True,\n",
        "            output_scores=True,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "        )\n",
        "    s = generation_output.sequences[0]\n",
        "    output = tokenizer.decode(s)\n",
        "    return output.split(\"### Response:\")[1].strip()\n",
        "\n",
        "\n",
        "gr.Interface(\n",
        "    fn=evaluate,\n",
        "    inputs=[\n",
        "        gr.components.Textbox(\n",
        "            lines=2, label=\"Instruction\", placeholder=\"Tell me about alpacas.\"\n",
        "        ),\n",
        "        gr.components.Textbox(lines=2, label=\"Input\", placeholder=\"none\"),\n",
        "        gr.components.Slider(minimum=0, maximum=1, value=0.1, label=\"Temperature\"),\n",
        "        # gr.components.Slider(minimum=0, maximum=1, value=0.75, label=\"Top p\"),\n",
        "        # gr.components.Slider(minimum=0, maximum=100, step=1, value=40, label=\"Top k\"),\n",
        "        gr.components.Slider(minimum=1, maximum=4, step=1, value=4, label=\"Beams\"),\n",
        "        gr.components.Slider(\n",
        "            minimum=1, maximum=512, step=1, value=256, label=\"Max tokens\"\n",
        "        ),\n",
        "    ],\n",
        "    outputs=[\n",
        "        gr.inputs.Textbox(\n",
        "            lines=8,\n",
        "            label=\"Output\",\n",
        "        )\n",
        "    ],\n",
        "    title=f\"🦙🌲🌸 Japanese-Alpaca-LoRA-{model_param}🌸\",\n",
        "    description=f\"Alpaca-LoRA is a {model_param}-parameter LLaMA model finetuned to follow instructions. It is trained on the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) dataset and makes use of the Huggingface LLaMA implementation. For more information, please visit [the project's website](https://github.com/tloen/alpaca-lora).\",\n",
        ").launch(inline=False ,share=True)"
      ],
      "metadata": {
        "id": "bAl6_6PrizMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W8hRrkPcO3JR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}